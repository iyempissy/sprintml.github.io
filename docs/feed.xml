<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2024-06-06T11:31:29+02:00</updated><id>/feed.xml</id><title type="html">SprintML Lab</title><subtitle>Website of the SprintML lab.
</subtitle><entry><title type="html">How to prompt LLMs with private data?</title><link href="/2024/04/27/private-prompts.html" rel="alternate" type="text/html" title="How to prompt LLMs with private data?" /><published>2024-04-27T20:30:00+02:00</published><updated>2024-04-27T20:30:00+02:00</updated><id>/2024/04/27/private-prompts</id><content type="html" xml:base="/2024/04/27/private-prompts.html"><![CDATA[<p><em>by Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch</em></p>

<p>Over the past few years, large language models (LLMs) have gained widespread attention in both the tech industry and academia, as well as the public at large. LLMs are capable of executing a wide range of language-related tasks, such as translation, text generation, and sentiment analysis. One particularly amazing property of LLMs is that they need only minor modifications to handle new tasks, making them particularly well-suited for the rapid development of new applications.</p>

<p>In many scenarios, a company might want to adapt these pretrained LLMs with data that contains sensitive information. For example, Epic, a healthcare software company in the U.S., is recently partnering with Microsoft to integrate GPT4 in managing patients’ electronic healthcare records. Adapting LLMs to this data naively could expose patients’ sensitive information. To prevent these privacy violations, we have to be careful how we construct the prompts we query the LLM with - as we will explain next.</p>

<h3 id="what-is-prompting-for-llms">What is ‘prompting’ for LLMs?</h3>

<p>Generally speaking, there are two standard ways to adapt LLMs to new tasks, “fine-tuning” and “prompting.” The first way, fine-tuning, updates the parameters of the LLM to better reflect the new task. The second, prompting, does not make any updates to the original LLMs. Instead, it adds examples to provide context for the input that the user submits. This is done directly when the model is asked to predict on an input from the new task.</p>

<p>A canonical type of a prompt contains a brief instruction of the task, followed by several examples of the task in the form of an input and output pair. For example, an LLM could be used to recognize whether the sentiment of a movie review was positive or negative using a prompt that looks like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Instruction: Given some text, classify whether it is positive or negative. 
Input: the movie was great.  
Output: positive
</code></pre></div></div>

<p>To get the model’s prediction on a new movie review, we would prepend the above prompt to the new query “Input: the film was rather boring.” The model would then output the following: “Output: negative”.</p>

<h3 id="why-prompting">Why prompting?</h3>

<p>Prompting offers several advantages over fine-tuning. First, prompting is very storage-efficient. Storing a separate fine-tuned model for each new task can be expensive because LLMs have lots of parameters. Instead, prompt tuning only requires storing a small prompt to provide task-specific context.</p>

<p>Second, the computation costs of fine-tuning become increasingly prohibitive as the models’ sizes grow. Prompting, as stated, works without tuning all of the model parameters.</p>

<p>Third, many LLMs are deployed behind APIs–a restricted interface for two or more computer programs to communicate with each other. This means that the end user does not have access to the model parameters. The only way to adapt the model in this case is through prompts.</p>

<h3 id="the-privacy-risks-of-prompting">The privacy risks of prompting</h3>

<p>When we started our work, little was known about the privacy implications of prompting. The first question we asked ourselves is: do LLM predictions leak sensitive information contained in their prompts?  To measure this, we carried out what is called a “membership inference attack” on model predictions. Given a particular example, this attack tries to figure out if the LLM used that example in its prompt when making predictions. This may be a very simple attack - but it can be used to construct much more sophisticated attacks like attacks that reconstruct the examples contained in prompts. Here are the results from this first experiment.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/prompts/miaprompt.png" width="90%" /></p>
</div>

<p>The left figure shows that the LLM is much more confident when being asked to predict on an input which it already appears in the prompt (as indicated in blue) than when it is asked to predict on an input that was not in the prompt (as indicated in orange). This means the adversary can “guess” that an example was in the prompt by looking at the examples for which the model is confident were in the prompt. The right figure shows that this guess is often a correct one. The ROC curve (receiver operating characteristic curve) of this attack demonstrates the adversary (indicated with the blue curve) is more likely to succeed than if it had guessed randomly (using a coin flip, as indicated by the red dotted line). Indeed, the further away the blue line is from the red line, the more successful the attack is.</p>

<p>Put altogether, this first experiment shows that there is significant risk when one includes sensitive data in the prompts that are given to LLMs. So, we set out to find a solution.</p>

<h3 id="privacy-preserving-prompts">Privacy-preserving prompts</h3>

<p>To protect privacy, intuitively, we would like to ensure that none of the examples contained in the prompt “influences” the outputs of the prompt too much. This way, when the LLM makes predictions from the prompt, its output won’t reveal too much information about the particular examples that were used to construct the prompt. It turns out that the research community has developed a framework to capture this intuition more rigorously - this framework is called differential privacy (DP). If you are interested in more details on differential privacy, you are welcome to check out <a href="https://www.cleverhans.io/2021/01/14/privacy-focus-algorithms.html">our earlier blog post on the topic</a>.</p>

<p>How can we construct prompts with differential privacy? First, we prompt the LLM with different prompts, each of which contains a disjoint subset of examples from the private dataset. Given an (unlabeled) input, each prompted LLM outputs a prediction based on its own prompt. Then we gather all the predictions from each prompted model and output the prediction that most models agree with. Intuitively, this “voting” process makes a prediction based on the information contained in multiple data points and reduces the influence that each data point has on the overall prediction made.</p>

<p>The above process alone is almost but not quite sufficient to provide differential privacy guarantees. To rigorously protect the privacy of the examples contained in the private set, we have to add noise to the models’ votes before we output the consensus. This approach is indeed well-established in the privacy-preserving machine learning literature. It is called Private Aggregation of Teacher Ensembles (PATE). For a detailed introduction to PATE, we refer you to <a href="https://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html">our another blog post</a>.</p>

<p>We then select the best pair of input and aggregated label and use that pair as the example that makes up our prompt for the LLM model. We refer to this prompt as the student prompt because this prompt was obtained by distilling the knowledge the teachers had acquired.</p>

<p>The diagram below illustrates this approach.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/prompts/promptpate.png" width="90%" /></p>
</div>

<p>We implement this PATE algorithm with two popular commercial LLM APIs, GPT3 and Claude. Our results show that the method offers very strong privacy protection for data contained in prompts as well as high prediction accuracy when using our private prompts. Our method is the first privacy-preserving algorithm that can be employed with all commercial APIs.</p>

<p>A subset of commercial APIs may provide additional access to the user. For instance, they may allow the user to compute gradients with respect to the inputs of the LLM being queried. When we have such access to the model, we can deploy another approach to obtain prompts that protect privacy.</p>

<p>These prompts are different to the ones we described until now. So far, we used what is called discrete prompts: the prompts were made up of real words. Yet, LLMs internally represent words as vectors of numbers. This can be exploited to prompt them more precisely. We call this type of prompts soft prompts, the LLM is prompted with vectors of numbers directly (rather than real words that are then later converted into vectors of numbers).</p>

<p>Since soft prompts are made of numbers, we can construct them using a search procedure that is based on gradient descent. Luckily, gradient descent is the most common way to train neural networks. Hence, the privacy community has devised numerous algorithms to make gradient descent differentially private. In particular, one algorithm stands out: differentially private stochastic gradient descent. It clips and noises gradients to make them private. Why do these extra steps protect the privacy of training data? Intuitively speaking, clipping the gradients ensures that an example cannot influence the model update too much, and adding noise obfuscates the particular updates that were applied to the model. Thus, by modifying the gradient descent procedure in training the soft prompts, the data used to train the soft prompts has a differential privacy guarantee.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/prompts/prmptdpsgd.png" width="50%" /></p>
</div>

<h3 id="conclusions">Conclusions</h3>

<p>Our work identifies the privacy risk of prompting and offers methods to construct discrete and soft prompts that preserve privacy. We find that prompting is a practical and efficient way to adapt LLMs with differential privacy. There are still many improvements that can be made to our algorithm. For example, it is interesting to see how to extend our approach to protect users against malicious LLM providers. We hope that our work will inspire others to work on this problem.</p>

<p>If you’d like to learn more, read our NeurIPS 2023 paper <a href="https://openreview.net/forum?id=u6Xv3FuF8N">“Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models”</a>. It goes into greater detail about our research on this topic.</p>]]></content><author><name></name></author><category term="private" /><category term="prompts" /><category term="LLMs" /><category term="DPSGD" /><category term="PATE" /><summary type="html"><![CDATA[by Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch]]></summary></entry><entry><title type="html">On stealing and defending self-supervised models</title><link href="/2023/02/23/model-extraction.html" rel="alternate" type="text/html" title="On stealing and defending self-supervised models" /><published>2023-02-23T00:00:00+01:00</published><updated>2023-02-23T00:00:00+01:00</updated><id>/2023/02/23/model-extraction</id><content type="html" xml:base="/2023/02/23/model-extraction.html"><![CDATA[<p><em>by Adam Dziedzic and Nicolas Papernot</em></p>

<p><em>This blog post is part of a series on model stealing, check out our previous blog posts
<a href="http://www.cleverhans.io/2021/04/28/is-this-model-mine.html">“Is this model mine?”</a>
for a general introduction to the problem and a new active defense against the extraction of supervised models <a href="http://www.cleverhans.io/2022/04/21/pow-defense.html">
“How to Keep a Model Stealing Adversary Busy?”</a>
.<a href="https://openreview.net/forum?id=EAy7C1cgE1L">[1]</a></em></p>

<p>Machine Learning (ML) models cater to tasks such as language translation, speech recognition, data annotation, or
optical character recognition.
The models that fulfill these tasks are publicly exposed via APIs. They are trained in either the
supervised or self-supervised setting. The supervised models are expensive in terms of data labeling but their training
cost is relatively inexpensive. On the other hand, the self-supervised models have zero cost of data labeling. They can
leverage all the available data points but such an approach increases the training cost of these models. The creators of
the models want to prevent
them from being stolen. In this blog post, we analyze how to steal and defend self-supervised
models <a href="https://arxiv.org/abs/2205.07890">[2]</a>
, <a href="https://arxiv.org/abs/2209.09024">[3]</a>.
The threat of stealing self-supervised learning models is real. An attacker’s
incentive is to steal a model at a much lower cost than training it from scratch. Recently, researchers showed that
training a ResNet50
SSL model costs north of $5713.92, whereas stealing such an encoder costs only around
$72.49 <a href="https://arxiv.org/abs/2201.11692">[4]</a>.</p>

<h3 id="self-supervised-learning">Self-Supervised Learning</h3>

<p>Self-supervised learning (SSL) emerges as a dominant learning paradigm behind modern ML APIs. The major paradigm shift
is that
these large self-supervised models, called encoders, are trained on a big amount of unlabeled data. These SSL models
return high-dimensional representations instead of low-dimensional outputs such as labels. The representations are
features extracted from a given input query and they can be used for many downstream tasks. For example, you can send a
block of text as input and receive an embedding as output, which is offered by an API exposed by
Cohere <a href="https://txt.cohere.ai/sentence-word-embeddings/">[5]</a>.
Going from supervised to self-supervised learning is essential and it is the future of the ML APIs. It is essential
since representations can be re-used on many tasks and you need a small number of labels to train downstream tasks.
A company that offers such an API does not have to know all of the labels required by a given client but they provide
a generic interface that extracts useful features for a given input.
First, we will show how to extract encoders and then present methods to detect if a given encoder is a stolen copy of a
victim encoder.</p>

<h3 id="how-to-steal-self-supervised-encoders">How to Steal Self-Supervised Encoders?</h3>

<p>The framework of our attacks is inspired by Siamese networks. For an input query, represented as an image of a (corgi)
dog (in the diagram below), we generate two augmentations by applying vertical flip and grayscale transformation.
Then, we query the victim encoder with the first augmentation and our stolen copy with the second augmentation. To steal
the victim encoder, we train the stolen encoder to minimize the loss between representations obtained from the victim
encoder \(y_1\) shown in blue, and representations from our stolen copy \(𝑦_2\) shown in red.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/ssl/siamese.png" width="90%" /></p>
</div>

<p>To this end, we apply contrastive loss functions (see the image below). In the representation space, before stealing,
the positive pairs (two
augmentations of the same image \(y_1^+\) and \(y_2^+\)) from
the victim and stolen encoders are far
from each other. After stealing, the positive pairs are close to each other. Thus, the stolen encoder replicates the
behavior of the victim encoder. The crucial point of contrastive loss functions is that the positive pairs stay close to
each other but also the negative examples (\(y_1^+\) and \(y_3^-\)) coming from different inputs are far from the
positive pairs.
The selection of the loss function is one of the most important hyper-parameter for stealing encoders. We compared
standard losses as well as modern batch contrastive losses. The standard loss like Mean Squared Error is used to
directly minimize distances between representations from the victim and the stolen copy. Modern batch contrastive losses
like InfoNCE <a href="https://arxiv.org/abs/2002.05709">[8]</a> or Soft Nearest
Neighbors <a href="https://proceedings.mlr.press/v97/frosst19a.html">[9]</a> compare not only positive but also negative pair
samples. We find that
contrastive losses
perform the best for both training and stealing encoders and allow us to decrease the number of stealing queries to be
less than 1/5th of the number of victim’s training data points.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/ssl/contrastive_loss.png" width="90%" /></p>
</div>

<h3 id="how-to-defend-or-detect-encoder-stealing">How to Defend or Detect Encoder Stealing?</h3>

<p>Having discussed how to steal encoders, let us consider defense methods. The active defenses either perturb or truncate
the answers to poison the training objective of an attacker but they were shown to harm substantially the performance of
legitimate users so they are not usable in the SSL setting <a href="https://arxiv.org/abs/2201.05889">[6]</a>. The watermarking
based defenses, for example, embed a unique task into the encoder, which marks the encoder as our property. If we can
prove that the stolen encoder also has our unique property, then we can detect a theft. During standard training of
encoders, we provide as inputs images and train the encoder to generate high-quality representations. For watermarking,
we embed the rotation task into encoders during training. As an example, the task is a binary classification where we
have to classify if a given image is rotated between 0 and 180 degrees or between 180 and 360 degrees. To implement the
watermark, we add the additional fully-connected layer on top of the representations. Whenever we tune the parameters
for the
rotation task, we also adapt all the other weights in the encoder. As a result, the watermarked encoder and its stolen
copies return not only embeddings but also the correct rotation range (see the schema below). We observe that for a
legitimately trained model, the watermark task achieves near-random accuracy of 50%. On
the other hand, for a stolen encoder, the more queries are used for extraction, the higher transfer of the watermark (&gt;
50%).
This holds across many loss functions used for stealing.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/ssl/watermark.png" width="90%" /></p>
</div>

<p>However, first, watermarking requires retraining or fine-tuning the large encoders, which is impractical. An adaptive
attacker can use different extraction methods to lower the transfer of the watermark from the victim to stolen encoders.
There exist many methods to remove watermarks. For example, in the case of the rotation watermark, an adversary can
obfuscate the representations to fool the detection of a watermark. To tackle the problems of watermarking, we propose a
new method to detect a stolen encoder. Our approach is based on dataset inference that treats the training data of a
victim encoder as its signature <a href="https://openreview.net/forum?id=hvdKKV2yt7T">[7]</a>. This detection method is effective
and does not require encoder fine-tuning.</p>

<p>For the resolution of the ownership, we assume that we as a defender have access to our training and test sets as well
as the query access to a suspect encoder. Training and test data come from the same distribution. The first step in our
method
is to train a
meta model, in this case a Gaussian Mixture Model (GMM) to estimate the distribution of representations. We use part of
the
traing set to do so. To resolve the ownership, we compare the log-likelihoods for the train vs test sets. In the case
when
the log-likelihood is much larger for the train set, we mark the tested encoder as stolen. Otherwise, if there is almost
no difference in the log-likelihood between the train and test sets, then such an encoder is marked as an independent
encoder. How do we quantify if the difference between the log-likelihoods of representations are significant? We do it
by harnessing statistical testing. Concretely, to verify
the ownership we use a statistical t-test. The null hypothesis is that there is no difference between the log-likelihood
for train and test sets. If the p-value is below a certain threshold (e.g., 1%,) then the encoder is
either the victim or stolen. Otherwise, the t-test is inconclusive and the suspect encoder is marked as independent,
meaning not stolen.</p>

<div style="text-align:center; margin-bottom:30px;">
  <p><img src="/assets/blog/ssl/dataset_inference.png" width="90%" /></p>
</div>

<h3 id="summary">Summary</h3>

<p>Let us summarize the main aspects stealing attacks and defenses for SSL models. Recent attacks on the SSL models show
that
high-quality encoders can be extracted at the fraction of the cost of creating the victim encoders.
We described how we can detect stolen self-supervised models by using the victim’s training and test data as the model’s
signature. The crucial intuition is that an encoder that was trained on the given training data or stolen from such
victim behaves differently on the training data vs the test data. On the other hand, an independently trained encoder
behaves similarly on both the train and test data. As of now, we can only detect stolen encoders. Defending encoders
against stealing attacks is challenging since representations leak large amounts of information and the existing
defenses cannot be applied out-of-the-box to protect encoders. Thus, there is a need to create active defenses for
self-supervised models which would not harm legitimate users but could increase the cost of encoder extraction.</p>

<h3 id="want-to-read-more">Want to read more?</h3>

<p>You can find additional details in our <a href="https://arxiv.org/abs/2205.07890">ICML paper</a> [2] and
the <a href="https://arxiv.org/abs/2209.09024">NeurIPS paper</a> [3]. The code for reproducing all our experiments is available in
our GitHub repositories: <a href="https://github.com/cleverhans-lab/ssl-attacks-defenses">SSLAttackDefenses</a>
and <a href="https://github.com/cleverhans-lab/DatasetInferenceForSelfSupervisedModels">DataSetInferenceForSelfSupervisedModels</a>
.</p>

<hr />
<p><em>[1] Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, Nicolas
Papernot. <strong><a href="https://openreview.net/forum?id=EAy7C1cgE1L">Increasing the Cost of Model Extraction with Calibrated Proof of Work</a></strong>
ICLR 2022.</em></p>

<p><em>[2] Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad Kaleem, Jonas Guan, Nicolas
Papernot. <strong><a href="https://arxiv.org/abs/2205.07890">On the Difficulty of Defending Self-Supervised Learning against Model Extraction</a></strong>
ICML 2022.</em></p>

<p><em>[3] Adam Dziedzic, Haonan Duan, Muhammad Ahmad Kaleem, Nikita Dhawan, Jonas Guan, Yannis Cattan, Franziska Boenisch,
Nicolas Papernot.
<strong><a href="https://arxiv.org/abs/2209.09024">Dataset Inference for Self-Supervised Models</a></strong> NeurIPS 2022.</em></p>

<p><em>[4] Tianshuo Cong, Xinlei He, Yang
Zhang. <strong><a href="https://arxiv.org/abs/2201.11692">SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders</a></strong>
CCS 2022.</em></p>

<p><em>[5] Luis Serrano. <strong><a href="https://txt.cohere.ai/sentence-word-embeddings/">What Are Word and Sentence Embeddings</a></strong>?</em></p>

<p><em>[6] Yupei Liu, Jinyuan Jia, Hongbin Liu, Neil Zhenqiang
Gong. <strong><a href="https://arxiv.org/abs/2201.05889">StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning</a></strong>
CCS 2022.</em></p>

<p><em>[7] Pratyush Maini, Mohammad Yaghini, Nicolas Papernot.
<strong><a href="https://openreview.net/forum?id=hvdKKV2yt7T">Dataset Inference: Ownership Resolution in Machine Learning</a></strong> ICLR
2021.</em></p>

<p><em>[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey
Hinton. <strong><a href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a></strong>
ICML 2020.</em></p>

<p><em>[9] Nicholas Frosst, Nicolas Papernot, Geoffrey
Hinton. <strong><a href="https://proceedings.mlr.press/v97/frosst19a.html">Analyzing and Improving Representations with the Soft Nearest Neighbor Loss</a></strong>
ICML 2020.</em></p>]]></content><author><name></name></author><category term="model" /><category term="extraction," /><category term="model" /><category term="stealing," /><category term="model" /><category term="functionality" /><category term="stealing," /><category term="self-supervised" /><category term="learning," /><category term="deep" /><category term="learning" /><summary type="html"><![CDATA[by Adam Dziedzic and Nicolas Papernot This blog post is part of a series on model stealing, check out our previous blog posts “Is this model mine?” for a general introduction to the problem and a new active defense against the extraction of supervised models “How to Keep a Model Stealing Adversary Busy?” .[1] Machine Learning (ML) models cater to tasks such as language translation, speech recognition, data annotation, or optical character recognition. The models that fulfill these tasks are publicly exposed via APIs. They are trained in either the supervised or self-supervised setting. The supervised models are expensive in terms of data labeling but their training cost is relatively inexpensive. On the other hand, the self-supervised models have zero cost of data labeling. They can leverage all the available data points but such an approach increases the training cost of these models. The creators of the models want to prevent them from being stolen. In this blog post, we analyze how to steal and defend self-supervised models [2] , [3]. The threat of stealing self-supervised learning models is real. An attacker’s incentive is to steal a model at a much lower cost than training it from scratch. Recently, researchers showed that training a ResNet50 SSL model costs north of $5713.92, whereas stealing such an encoder costs only around $72.49 [4]. Self-Supervised Learning Self-supervised learning (SSL) emerges as a dominant learning paradigm behind modern ML APIs. The major paradigm shift is that these large self-supervised models, called encoders, are trained on a big amount of unlabeled data. These SSL models return high-dimensional representations instead of low-dimensional outputs such as labels. The representations are features extracted from a given input query and they can be used for many downstream tasks. For example, you can send a block of text as input and receive an embedding as output, which is offered by an API exposed by Cohere [5]. Going from supervised to self-supervised learning is essential and it is the future of the ML APIs. It is essential since representations can be re-used on many tasks and you need a small number of labels to train downstream tasks. A company that offers such an API does not have to know all of the labels required by a given client but they provide a generic interface that extracts useful features for a given input. First, we will show how to extract encoders and then present methods to detect if a given encoder is a stolen copy of a victim encoder. How to Steal Self-Supervised Encoders? The framework of our attacks is inspired by Siamese networks. For an input query, represented as an image of a (corgi) dog (in the diagram below), we generate two augmentations by applying vertical flip and grayscale transformation. Then, we query the victim encoder with the first augmentation and our stolen copy with the second augmentation. To steal the victim encoder, we train the stolen encoder to minimize the loss between representations obtained from the victim encoder \(y_1\) shown in blue, and representations from our stolen copy \(𝑦_2\) shown in red. To this end, we apply contrastive loss functions (see the image below). In the representation space, before stealing, the positive pairs (two augmentations of the same image \(y_1^+\) and \(y_2^+\)) from the victim and stolen encoders are far from each other. After stealing, the positive pairs are close to each other. Thus, the stolen encoder replicates the behavior of the victim encoder. The crucial point of contrastive loss functions is that the positive pairs stay close to each other but also the negative examples (\(y_1^+\) and \(y_3^-\)) coming from different inputs are far from the positive pairs. The selection of the loss function is one of the most important hyper-parameter for stealing encoders. We compared standard losses as well as modern batch contrastive losses. The standard loss like Mean Squared Error is used to directly minimize distances between representations from the victim and the stolen copy. Modern batch contrastive losses like InfoNCE [8] or Soft Nearest Neighbors [9] compare not only positive but also negative pair samples. We find that contrastive losses perform the best for both training and stealing encoders and allow us to decrease the number of stealing queries to be less than 1/5th of the number of victim’s training data points. How to Defend or Detect Encoder Stealing? Having discussed how to steal encoders, let us consider defense methods. The active defenses either perturb or truncate the answers to poison the training objective of an attacker but they were shown to harm substantially the performance of legitimate users so they are not usable in the SSL setting [6]. The watermarking based defenses, for example, embed a unique task into the encoder, which marks the encoder as our property. If we can prove that the stolen encoder also has our unique property, then we can detect a theft. During standard training of encoders, we provide as inputs images and train the encoder to generate high-quality representations. For watermarking, we embed the rotation task into encoders during training. As an example, the task is a binary classification where we have to classify if a given image is rotated between 0 and 180 degrees or between 180 and 360 degrees. To implement the watermark, we add the additional fully-connected layer on top of the representations. Whenever we tune the parameters for the rotation task, we also adapt all the other weights in the encoder. As a result, the watermarked encoder and its stolen copies return not only embeddings but also the correct rotation range (see the schema below). We observe that for a legitimately trained model, the watermark task achieves near-random accuracy of 50%. On the other hand, for a stolen encoder, the more queries are used for extraction, the higher transfer of the watermark (&gt; 50%). This holds across many loss functions used for stealing. However, first, watermarking requires retraining or fine-tuning the large encoders, which is impractical. An adaptive attacker can use different extraction methods to lower the transfer of the watermark from the victim to stolen encoders. There exist many methods to remove watermarks. For example, in the case of the rotation watermark, an adversary can obfuscate the representations to fool the detection of a watermark. To tackle the problems of watermarking, we propose a new method to detect a stolen encoder. Our approach is based on dataset inference that treats the training data of a victim encoder as its signature [7]. This detection method is effective and does not require encoder fine-tuning. For the resolution of the ownership, we assume that we as a defender have access to our training and test sets as well as the query access to a suspect encoder. Training and test data come from the same distribution. The first step in our method is to train a meta model, in this case a Gaussian Mixture Model (GMM) to estimate the distribution of representations. We use part of the traing set to do so. To resolve the ownership, we compare the log-likelihoods for the train vs test sets. In the case when the log-likelihood is much larger for the train set, we mark the tested encoder as stolen. Otherwise, if there is almost no difference in the log-likelihood between the train and test sets, then such an encoder is marked as an independent encoder. How do we quantify if the difference between the log-likelihoods of representations are significant? We do it by harnessing statistical testing. Concretely, to verify the ownership we use a statistical t-test. The null hypothesis is that there is no difference between the log-likelihood for train and test sets. If the p-value is below a certain threshold (e.g., 1%,) then the encoder is either the victim or stolen. Otherwise, the t-test is inconclusive and the suspect encoder is marked as independent, meaning not stolen. Summary Let us summarize the main aspects stealing attacks and defenses for SSL models. Recent attacks on the SSL models show that high-quality encoders can be extracted at the fraction of the cost of creating the victim encoders. We described how we can detect stolen self-supervised models by using the victim’s training and test data as the model’s signature. The crucial intuition is that an encoder that was trained on the given training data or stolen from such victim behaves differently on the training data vs the test data. On the other hand, an independently trained encoder behaves similarly on both the train and test data. As of now, we can only detect stolen encoders. Defending encoders against stealing attacks is challenging since representations leak large amounts of information and the existing defenses cannot be applied out-of-the-box to protect encoders. Thus, there is a need to create active defenses for self-supervised models which would not harm legitimate users but could increase the cost of encoder extraction. Want to read more? You can find additional details in our ICML paper [2] and the NeurIPS paper [3]. The code for reproducing all our experiments is available in our GitHub repositories: SSLAttackDefenses and DataSetInferenceForSelfSupervisedModels . [1] Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, Nicolas Papernot. Increasing the Cost of Model Extraction with Calibrated Proof of Work ICLR 2022. [2] Adam Dziedzic, Nikita Dhawan, Muhammad Ahmad Kaleem, Jonas Guan, Nicolas Papernot. On the Difficulty of Defending Self-Supervised Learning against Model Extraction ICML 2022. [3] Adam Dziedzic, Haonan Duan, Muhammad Ahmad Kaleem, Nikita Dhawan, Jonas Guan, Yannis Cattan, Franziska Boenisch, Nicolas Papernot. Dataset Inference for Self-Supervised Models NeurIPS 2022. [4] Tianshuo Cong, Xinlei He, Yang Zhang. SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders CCS 2022. [5] Luis Serrano. What Are Word and Sentence Embeddings? [6] Yupei Liu, Jinyuan Jia, Hongbin Liu, Neil Zhenqiang Gong. StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning CCS 2022. [7] Pratyush Maini, Mohammad Yaghini, Nicolas Papernot. Dataset Inference: Ownership Resolution in Machine Learning ICLR 2021. [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton. A Simple Framework for Contrastive Learning of Visual Representations ICML 2020. [9] Nicholas Frosst, Nicolas Papernot, Geoffrey Hinton. Analyzing and Improving Representations with the Soft Nearest Neighbor Loss ICML 2020.]]></summary></entry></feed>